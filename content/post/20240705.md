---
title: "Adam 优化器; LLM 角色指令增强; Step-DPO .etc"
date: 2024-07-05T19:31:56+08:00
tags: ["优化", "角色扮演", "DPO"]
---


## Adam 优化器 State 的显存占用; LLM 角色指令增强的理论分析; Step-DPO 提升 LLM 数学推理能力; MInference 加速 Long-context LLMs 推理; Dense 模型与 MoE 模型的等效性

**1. Adam 优化器 State 的显存占用**:  当GPU卡数足够多时，Adam 优化器的 state 不会成为通信和显存的瓶颈
*  Adam 的 state 不涉及卡间通信，都是卡内通信
*  卡越多，Adam state 占总显存的比例越小
*  减少优化器本身显存的工作，通常只在少卡的场景才有价值
*  相关引用：
    * **文章**: [Adam-Mini](https://arxiv.org/abs/2402.00658) 

**2. LLM 角色指令增强的理论分析**:  如何从理论角度分析 LLM 中角色指令对模型能力的增强
*  目前缺乏相关的理论分析，只有实验结果
*  可以借鉴 ICL 的理论分析方法
*  相关引用：
    * **文章**: [Characterizing Roles for Instruction Following](https://arxiv.org/pdf/2305.16367) 
    * **文章**: [Want to Predict/Explain/Control the Output of GPT-4? Then This Post is For You](https://www.lesswrong.com/posts/G3tuxF4X5R5BY7fut/want-to-predict-explain-control-the-output-of-gpt-4-then)

**3.  Step-DPO 提升 LLM 数学推理能力**:  使用 Step-DPO 方法，仅需少量数据和训练步骤即可提升LLM数学推理能力
*  在 Qwen2-7B-Instruct 上微调，MATH 数据集上 0-shot CoT 准确率从 53.0% 提升至 58.6%
*  在 Qwen2-72B-Instruct 上训练，MATH 数据集准确率从 59.7% 提升到 70.8%，超过 GPT-4-1106 等模型
*  相关引用：
    * **代码**: [Step-DPO](https://github.com/dvlab-research/Step-DPO)
    * **Demo**:  http://103.170.5.190:7870/
    * **数据**: [Math-Step-DPO-10K](https://huggingface.co/datasets/xinlai/Math-Step-DPO-10K)
    * **模型**: [Step-DPO Models](https://github.com/dvlab-research/Step-DPO?tab=readme-ov-file#models)

**4.  MInference 加速 Long-context LLMs 推理**: 利用动态稀疏注意力机制加速 Long-context LLMs 在 prefilling 阶段的推理速度
*  将 Long-context Attention 存在的动态稀疏性归纳为三种 pattern，并通过离线搜索确定最优 pattern
*  结合动态稀疏编译器和 GPU 运算，实现 1M tokens pre-filling 阶段 10 倍加速，并保持与 full attention 相同的性能
*  相关引用：
    * **项目主页**: [MInference](https://aka.ms/MInference)


**5.  Dense 模型与 MoE 模型的等效性**:  相同参数量情况下，MoE 模型是否能达到 Dense 模型的性能
*  MoE 模型依靠 routing 的灵活性，在激活参数量相同的情况下，性能优于 Dense 模型
*  MoE 模型相当于多个 well-trained 的模型替补上场，而 Dense 模型只有一个
*  追求 MoE 和 Dense 模型的中间临界点没有太大意义，更重要的是根据计算需求设计合适的模型结构
*  可以通过改变 routing 的 distribution 来适应不同的计算需求
*  可以通过实验比较相同数据量和训练配置下，MoE 模型的最终效果与多大参数量的 Dense 模型等效
*  相关模型:
    * **Mixtral-8x7B**: 激活 13B 参数，总参数 46B，等效于一个 25B 的 Dense 模型
    * **DeepSeekV2**: 激活 21B 参数，总参数 236B，等效于一个 90B 的 Dense 模型

**6.  Exposure Bias 对 LLM 模型训练的影响**:  随着模型规模的增大，Exposure Bias 问题是否依然存在
*  一种可能是模型的表征能力更强，generalization 能力更好，Exposure Bias 的影响变小
*  另一种可能是 Autoregressive 模型本身就是真实分布的一致估计量，数据量足够大时能够收敛到真实分布，从而消除 Exposure Bias
*  相关观点:
    *  LLM 的规模和数据量不断增大，repetition 等问题依然存在
    *  PPO 算法可以缓解 Exposure Bias 带来的问题
    *  SFT 过程中出现的 overfitting 问题可能与 Exposure Bias 相关

**7.  论文提交时间与影响力**:  论文提交时间对论文的影响力有多大
*  提交时间越早，排名越靠前，曝光率越高
*  大 V 转发能够显著提高论文的曝光率和影响力
*  最终决定一篇论文影响力的因素是论文本身的价值
*  相关工具：
    * **网站**: [arXiv Local Time](https://arxiv.org/localtime)

**8. LLM 模型中 Query Tokens 的 Masking**:  在预训练阶段屏蔽疑似 query 的 tokens 是否会影响模型的能力
*  可能会影响模型提出问题的能力
*  预训练数据中并非所有数据都是 query + answer 的格式，难以确定 query 的范围


## Quick points：
1.  **RNN 模式推理框架**:  Mamba 或其他 state-space 模型是否有现成的 RNN 模式推理框架
2.  **MLLM 评估**:  BLINK 是一个新的 MLLM 评估基准，关注 14 个视觉感知任务
3.  **数学推理能力提升**:  Step-DPO 方法能够显著提升 LLM 的数学推理能力
4.  **Long-context LLMs 推理加速**:  MInference 利用动态稀疏注意力机制加速 Long-context LLMs 推理
5.  **MoE 专家数量**:  DeepSeek 模型有很多专家并且性能强大，但专家数量过多会导致路由性能下降
6.  **Greedy Decoding**:  实际应用中为什么大家不使用 Greedy Decoding
7.  **Llama3 批量生成**:  如何使用 Llama3-instruct 进行批量生成
8.  **Context 中插入 Textbook**:  在 Context 中插入 Textbook 对 LLM 模型回答问题准确性的影响