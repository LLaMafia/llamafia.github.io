---
title: "大模型缺陷"
date: 2023-12-17T19:31:56+08:00
tags: ["大模型缺陷", "多轮对话", "KV cache 压缩"]
---


## 大模型缺陷：

文章：Berglund, Lukas, et al. "The Reversal Curse: LLMs trained on" A is B" fail to learn" B is A"." arXiv preprint arXiv:2309.12288(2023).

观察到的大模型缺陷如下：让模型在 A is B 数据上训练时，并不能使得模型泛化到 B is A。主要体现到的形式有两种：对于模型进行微调的时候，如果训练 A is B，那么再提问 B is ? 则成功率较低。对于已有模型进行测试时，发现其对于 (A, relation, B) 这样知识的掌握程度具有不对称性。

研究员一: 目前认为A is B不能让A的embedding靠近B的embedding，就像1 + 1 = 他会输出2，但 2 = 应该不会输出1 + 1。不过在大模型下怎么能让Q1和Q2学习语义等价，也就是embedding相似呢？

研究员二：entailment is not similarity，这也是 retrieval 的一个重要问题。

研究员三：模型有足够冗余就不需要embedding接近。embedding接近更适合retrieval和classification；当然我们也可以说generation其实就是multi-label classification over the vocab；reverse curse是正常的；

研究员四：semantics上reverse curse是正常呀，比如finetune“A是B的小孩”，然后你问”B的小孩是谁”，那可能是A也可能是C呀（因为可能不只一个孩子） ；因为人类也不是完全双向的。

研究员五：对于思维链的解决，chain-of-thought等价于beam search的时候beam size足够大，跳出local maximum，搜索了更大的生成空间。前提是 CoT 的结果比直接回答的结果好。CoT中间步骤对于问题其实都是错的，是很差的结果，所以beam size不够大就会丢掉，等不到最后的正确答案。加一个cot的prompt其实强制让模型把直接出答案的score也降低了。所以如果beam size足够大，模型自己也能发现cot的过程。

研究员六：cot是从seqlen维度增加模型复杂度的手段，因为很多时候单靠模型和原始输入，模型复杂度不够。

研究员七：人也只有一个人脑，所以不管从优雅、效果还是拟人的角度，都应该一个模型解决所有事情。都塞入模型不意味着背下来，像transformer的kv cache，就是放在那里等待检索而已，反而各种常数复杂度的RNN模型才能死记硬背的体现。假如transformer的context length做到了100M，那我把前99M都用来编码维基百科（或者其他你感兴趣的知识），提前算好kv cache存下来，后面1M才用来自由生成，这有何缺点？为什么不追求？当context length的成本不是问题时，RAG就没有意义，RAG本质上就是用简化的方法去增加当前llm的context length，这是因为当前的context length成本很大。

研究员八：但是RAG计算成本低。本质上是召回与精排，相当于预处理。上下文本长，计算成本高，至少目前的llm是这样，这两个其实就是权衡。推理时成本高还是训练时成本高。

## 多轮对话&& KV cache压缩
Q: 请问多轮对话这个事情是如何实现的呢？有没有博客讲述这个事情，这种能力一般用什么 benchmark 衡量呢？

研究员一: mtbench 会测二轮; 对话的考察维度太多。通用的就是对抗评测，让gpt4来胜平负打分。有些专项的评测，比如检查对话是否重复生成，就建一些专门的测试集; 单向GPT模型结构，有KV cache，前面的对话计算后缓存了，只需要计算新输入这轮，就比较快

研究员二: 还真不是，上一轮对话和新一轮对话之间的时间间隔不可预料，长期保存在显卡中很浪费显存的，所以一般新一轮对话都是重新计算的，不会cache; 只有同一轮对话的生成过程中，才会cache.

研究员三: 谢谢指正，前面说得不准确，主要费时还是decoder环节，context编码只一次还是比较快的；不过对于有些对话应用场景，预先设置的固定prompt很长的，这部分是要做kvcache的

研究员四：这部分的 cache 是不是也能 somehow 从一开始就压缩？like 输入 100k token 然后 somehow 压到 10k

研究员五：这个方向可能会比linear transformer更promising, 从information entropy/flow 角度一定是极度冗余的，且是非常dynamic和层间不均衡的，从n^2 减看起来会比直接构建一个static的sparse attention会更有可能一点

研究员六：H2O paper就是做token dropping的，发现去掉80%的KV cache token，效果还是很好，甚至generation更加creative。StreamingLLM的动机其实也是想看H2O按照cumulative attention score criterion，究竟丢掉了哪些token，结果发现前四个token的分数异常高；然后居然保留了前四个token就可以一直聊天聊下去。

研究员七：这个结果在这里也得到了验证：https://zhuanlan.zhihu.com/p/619703849。另外，scissorhand的结果说明，即某些绝对位置的token会一直持续被attend (注意力相对较高）

研究员八：StreamingLLM是绝对位置，但H2O drop的token是按照某个准则动态计算出来的；用StreamingLLM的position encoding的技巧，也可以搞StreamingH2O，效果比drop固定位置的token更好 https://allenz.work/?p=11

## 论文分享

### 《Zoology: Measuring and Improving Recall inEfficient Language Models》
系统比较卷积、线性RNN与注意力模型的论文来了
https://arxiv.org/pdf/2312.04927

**FAQ by Kimi Chat**

Q: 这篇论文试图解决什么问题？

根据文件内容，这篇论文主要关注和分析了近期流行的注意力无关的语言模型架构，特别是结合了门控和卷积的语言模型。论文指出虽然这些架构在某些方面已经接近或达到了基于注意力的模型的性能，但在语言建模任务上仍然存在一定的困惑性差距。论文通过系统地评估和分析不同的注意力无关和注意力模型，发现这种差距主要源于这些模型在处理上下文中重复出现的词汇和短语时的性能不足。论文提出了一个新的正式任务“多查询关联回忆”(Mqar)来更好地反映这种能力。通过分析Mqar任务，论文揭示了基于门控的卷积模型在高效处理可变距离的token-to-token交互方面的局限性，并指出了需要数据依赖的序列混合来解决Mqar任务。总的来说，这篇论文试图通过深入分析和理论探究，解释基于注意力和非注意力架构在语言建模任务上的性能差距，并为改进非注意力模型提供指导。

Q: 总结一下论文的主要内容

论文主要做了以下几件事：1. 在Pile语料库上预训练和评估了多种注意力和基于门控卷积的语言模型，发现基于门控卷积的模型在语言建模质量上仍然落后于基于注意力的模型。2. 通过细粒度分析，发现基于门控卷积的模型在处理上下文中重复出现的词汇时性能较差，这解释了它们在语言建模上的差距。3. 提出了一个新的合成任务“多查询关联回忆”(Mqar)来反映语言建模中的关联回忆能力。4. 从理论和实证两个角度分析了基于门控卷积的模型在Mqar任务上的局限性，比如模型维度需要随序列长度线性增长，而注意力模型可以对Mqar任务进行更高效处理。5. 在基于门控卷积的模型中引入输入相关的稀疏注意力机制，使得这些模型在Mqar任务和语言建模上的性能显著提升，接近甚至超过基于注意力的模型。6. 证明了通过设计合适的输入相关的稀疏注意力机制，基于门控卷积的模型可以在保持次线性计算复杂度的同时，实现对Mqar任务的高效处理。7. 使用合成的Mqar数据作为训练信号，可以帮助改进基于门控卷积的模型在语言建模上的性能。总体来说，论文通过理论分析和实证研究，解释了基于门控卷积的模型在语言建模任务上与基于注意力模型之间的能力差距，并指出了通过设计合适的输入相关的稀疏注意力机制可以提升这些模型在关联回忆任务和语言建模上的性能。

### 《注意力机制真的懂得“集中注意力”吗？》
https://kexue.fm/archives/9889

本文提出了通过Attention矩阵的稀疏程度来考察不同Attention机制潜力的思路，得出二次型Attention机制有可能实现任意稀疏的Attention矩阵，线性Attention则并不容易实现这种稀疏，或者只能实现绝对位置相关的稀疏，这可能是线性Attention能力有所限制的原因之一。

https://mp.weixin.qq.com/s/GZ2a9SLhLR5C1Vmp6NLbjg


## 讨论&杂谈
D: 如果lora adapter, 可以随意切换, 而且他们的开销也很小, 那是不是可以直接弄好几个adapter来当作moe

F: https://github.com/S-LoRA/S-LoRA, S-LoRA: Serving Thousands of Concurrent LoRA Adapters, Ion Stoica组做了

D: 如果Adapter可以合回原模型应该很有意思的，DARE那种setting，但是不是合LLM而是合adapter回LLM，成本又低也可以迭代

F: 之前有看到这种 combine adapter 的思想在图像生成里的应用（weight arithmetic），比如 ZipLoRA。好奇 LLM 里这么做目的是什么呢，参数太小也不足以更新知识，或许 align 打补丁？

F1: 其实ICL到一些具体的NLP tasks上还挺有差距的，个人感觉从SFT到extreme multitask learning还是有探索的价值，个人的一些不成熟的想法

F2: Adapter还是能把performance怼上去的，而且最近其实是有一些文章说ICL和SFT其实本质是定位到一个小的subnet来处理任务，那如果有更好的adpater加强这部分subnet，这个直觉上也比较make sense

F3: 有篇 ICML'23 的论文（ https://github.com/joeljang/ELM）讨论过类似的 multi-task expert

D: https://openai.com/research/weak-to-strong-generalization, 这有点颠覆我的想法;

F: 目前认为weak learner相当于一个有损压缩器

F1: 其实这个类似的思路contrastive decoding中就有了, 大模型犯的错误的pattern，在小模型中只会更常见，所以可以直接做contrast来detect大模型的问题.



## 简短问答

Q: 如果 moe 和 dense 都是总量 70b，moe activation 有 20b 但 dense activation 是 70b；这种情况两者 latency 和 throughput 会差多少啊？

A: https://twitter.com/dzhulgakov/status/1734029842436472898?t=KtvJCcKS32V7qPS7o_wMuQ&s=19; 这个讲的还挺对的.

A1: 当query很多能把所有expert打满的话此时moe效率是最高，tflops等于同参数dense，但moe单个query计算量只有dense的大约1/3，因此吞吐量会大三倍

Q: 请问 PIT 是哪个工作？

A: https://dl.acm.org/doi/abs/10.1145/3600006.3613139, 是和Megablocks同期的工作

Q: https://twitter.com/rasbt/status/1734920232173539796 这几条结论大家有认同的吗?

A: repeat data那个的话还是看sasha的那篇scaling laws，正好也是neurips runner-up，值得一读. 这个应该是这篇 https://arxiv.org/abs/2305.16264

A1: repeat data  anthropic的scaling law也有分析过https://arxiv.org/abs/2205.10487

Q: 有篇论文讲的是 先让模型回答prompt涉及的背景知识或上下文，然后再根据上下文回答问题，效果会更好，大家有记得这是哪篇论文吗?

A: https://mp.weixin.qq.com/s/VNUAkcp20J6ZEwxwPyMeCA;可以参考这个：https://mp.weixin.qq.com/s/tPo3J0n89fVADMppkgOlyQ

Q: 请问fine tune LLMs后做inference，一般用什么decoding策略（sampling、beam search、etc），有什么讲究吗？还是说不同数据用不同方式呢？

A: contrastive decoding 吧, 对于需要 fatithful 的场景会好一些; 原始论文 https://arxiv.org/pdf/2210.14140.pdf 实际用起来，能比 beam search 长一些，比nucleaus decoding 随机性小一些，但是都有限.




