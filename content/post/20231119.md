---
title: "Learning Rate 和 Batch Size 的关系"
date: 2023-11-19T19:31:56+08:00
tags: ["Learning Rate", "Batch Size", "多卡", "Grok-1"]
---

## Learning Rate 和 Batch Size 的关系
现代的大模型的 batch size 都是如何调整的呢？感觉 llama2 无论如何都是 4M tokens 一个 batch，这是有什么深思熟虑，还是就是为了显存呢？
* 跟显存无关。4M tokens 也是经过梯度累加的， acc step 增加不会增加显存
* 从 Infra 的角度，增加 Global Batch Size 总可以提升训练速度，因为减少了模型更新的总次数。 只要能保证模型收敛的效果， batch size 越大越好
* 在 OpenAI [An Empirical Model of Large-Batch Training](https://arxiv.org/abs/1812.06162) 中提到
* 大致遵循平方根关系

## 多机多卡并行方案
* 如果是多机多卡且卡间有 NVLink 链接，机间以 InfiniBand链接，则 transformer trainer + deepspeed zero++ HSDP + flash attention + bf16 + activation checkpointing 是一个不错的配置，适用于模型经过 zero3 sharding 之后可以装进一台机器那张卡的情况
* Megatron-LLM （一个支持了LLaMA的Megatron-LM的fork）；长seqlen的情况下，开sequence parallel + gradient checkpoint, 4xA100 40G就就能上16k的window
* [HSDP](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019): 一台 node 内部模型做 fsdp，不同 node 之间做 ddp. Jax [EasyLM](https://github.com/young-geng/EasyLM) 有实现
* Jax在这一点很方便，all dp、all fsdp、fsdp + ddp、fsdp+tp随意切换，就一两行代码的改动。

## Grok-1 中匈牙利考试数据集

* 可以用来[测试模型有没有刷分](https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam)
* GPT4 也在 GSM8K / MATH 上训练过

## 推荐论文的 Agent 
每天刷 Arxiv 不敢懈怠，问题：能否做一个 Agent 用来每天推荐优质论文？
* [Tatsu 的 Lab](https://tatsu-lab.github.io/gpt_paper_assistant/) 做了一个
* [Aminer](https://www.aminer.cn/) 也推出了一个，按添加订阅按钮即可

## RNN 类模型
* rwkv 的 Inference 速度是 llama 的几倍以上，如果确实效果好的话，能大幅降低推理成本
* 从multi-head线性attention的角度开，rwkv就是num_heads=hidden_size，head_dim=1的选取，这个组合自然是最快的（attention搞这个组合也快），但记忆容量也最差，所以rwkv5搞回了head_dim > 1的multi-head，但随之速度也不可能这么快了
* 记忆向量有大有小，比如1000维的记忆向量，跟1000^2维的记忆向量，速度肯定不一样的
* RNN只是理论上能以每一步固定的推理成本处理任意长的seqlen，但不是“能支持”，首先有限记忆容量对于足够long的context肯定有问题，其实就算看外推，它的外推效果也不好，所以无限长并不存在。。。
* 目前看来，rnn类模型没有解决的一个问题就是对prompt的形式依赖很大，换一下顺序效果就差很多。
* 比如做阅读理解，“材料 + 问题”作为prompt效果就比较差，“问题+材料”作为prompt效果就比较好。因为说白了RNN就是靠背，“材料 + 问题”意味着你先要把材料背下来，然后再看问题回答，这其实就不符合人的做题规律了

## 涌现能力的原理，小模型可以吗？
研究员 1
* 一种解释是，大模型的涌现能力可能是某些任务完成依赖的前置（简单）任务较多，模型能力表现为涌现，under the hood, 依赖的前置任务能力一直在增强，直到达到一个质变的点。
* 蒸馏小模型时直接学习大模型的distribution，直觉上信息量比单个token的尖峰分布更大，确实有机会学得更好。
* 能否学到大模型的涌现出的能力，跟模型容量有关系，太小的模型能可能丢失大模型分布中一些subtle的detail。宏观表现可能是常见任务可以蒸馏得不错，但囿于模型capacity，细节的任务可能表现不佳，最终结果是有些高难度任务只有大模型能解决。
* 可以类比照片的分辨率，蒸馏得到的小模型类比为高分辨率图像的thumbnail，可以大致看出轮廓，但缺少细节。

研究员 2
* sanjeev arora他们用random graph theory的threshold phenomenon来解释的theory:
* 我们可以认为predict next word token的时候，其实隐式的在学习做各种task，这就能build一个很大的graph，word和task相连，越大的model越有capacity来存这个图
* 当model训到一定程度的时候，那么很多graph的性质就会突然涌现(比如出现很多clique)，宏观表现就是虽然没训某些task，但是因为图已经大概率连通了，所以model能做
* 按照这个theory的话，小模型蒸馏只是从大的random graph里sample一个子图

研究员 3
* 另一个更简单的例子是coupon collector problem, 要收集N个不同的coupon，每去一次店里随机给一个coupon，那要去几次才能收集全？
* 去N次大概率收集不全，但如果去O(N log N)次，那么大概率能收集全

研究员 4
* 如果认为数据里对应的技能点有一个 dependency graph，那么一个天然的推论是如果能对数据做拓扑排序，技能点前置摆在前面，应该就能实现预训练速度最大化 

相关材料
* A Theory for Emergence of Complex Skills in Language Models. [Arxiv](https://arxiv.org/abs/2307.15936)
* Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models. [Arxiv](https://arxiv.org/abs/2307.14430)

## 讨论

Q：学的广是不是学得精的必要条件？

A：现代科研中，广度是深度的坚实基础。尤其是 LLM 研究，从底层分布式实现到高层人物抽象，想做好研究不得不全栈

Q：可以民科风格博取流量吗？

A：
* 这种流量容易被反噬
* 你希望别人因为你工作 solid 讨论虚心而记住你，不希望别人因为你民科记住你

Q: 用弱模型监督强模型，这个事情会 make sense 么？

A: 
* 如果弱模型能够ensemble出来强模型，那么弱模型能够监督强模型感觉也不会让人惊讶
* 这里的核心 assumption 是判别任务，虽然也不简单，但是 in general，要比生成任务简单一些
* agent进化过程也可以参考 RL league training, 一群更”弱”的agents从不同角度帮助main agent进化更强更鲁棒，就像顶级运动员有体能教练，技术教练，心理教练
