---
title: "Adam 优化器在多卡训练中的通信问题; LLM 角色指令增强的理论分析;  MCTS 在 RAG 中的应用"
date: 2024-07-10T19:31:56+08:00
tags: ["Adam 优化器", "LLM 角色指令增强", "MCTS", "RAG", "Step-DPO"]
---


## 1. Adam 优化器在多卡训练中的通信问题
* **问题**: Adam 优化器状态的卡间通信问题
* **观点**: 当 GPU 数量足够多时，Adam 优化器的状态信息并不会成为通信瓶颈，因为模型参数在每张卡上占的比例很低，且状态更新是 element-wise 的，无需卡间通信。 
* **结论**: 减少优化器自身显存的工作，只在少卡场景才有价值，Adam-mini 在多卡场景下对显存的减少有限
* 相关引用:
  * **论文**: Adam-mini

## 2. LLM 角色指令增强的理论分析
* **问题**:  寻找 LLM 中角色指令增强模型 (例如：你是一个优秀的软件工程师)  在专业方面能力的理论分析论文
* **讨论**: 群友认为很难形式化，现有的论文都只有实验结果，缺乏理论分析和假设
* **相关工作**:
    * **论文**: [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/pdf/2305.16367)
    * **博客**: [Want to predict, explain & control the output of GPT-4? Then...](https://www.lesswrong.com/posts/G3tuxF4X5R5BY7fut/want-to-predict-explain-control-the-output-of-gpt-4-then)  

## 3. MCTS 在 RAG 中的应用
* **问题**:  MCTS除了在强化学习中用于构建 CoT 数据，在 RAG 方向有没有什么实践
* **方向**: 有论文提到了使用 MCTS 优化检索，但具体细节未找到


## 4. Step-DPO: 用少量数据提升 LLM 数学推理能力
* **方法**: Step-DPO 通过仅使用 10K 数据和几百步训练，就能显著提升 LLM 的数学推理能力
* **结果**: 在 Qwen2-7B-Instruct 上，Step-DPO 将 MATH 数据集上的 0-shot CoT 准确率从 53.0% 提升至 58.6%；在 Qwen2-72B-Instruct 上，准确率从 59.7% 提升到 70.8%，超过了 GPT-4-1106、Gemini-1.5-Pro 和 Claude-3-Opus 等模型
*  **相关引用**:
    * **代码**: [https://github.com/dvlab-research/Step-DPO](https://github.com/dvlab-research/Step-DPO)
    * **Demo**: [http://103.170.5.190:7870/](http://103.170.5.190:7870/)
    * **数据**: [https://huggingface.co/datasets/xinlai/Math-Step-DPO-10K](https://huggingface.co/datasets/xinlai/Math-Step-DPO-10K)
    * **模型**: [https://github.com/dvlab-research/Step-DPO?tab=readme-ov-file#models](https://github.com/dvlab-research/Step-DPO?tab=readme-ov-file#models)

## 5.  训练数据中 Query Tokens 的 Masking 对 LLM 预训练的影响
* **问题**: 在预训练阶段，是否应该 mask 掉疑似 query 的 tokens
* **挑战**: 
    * 预训练数据并非都是 query + answer 的格式
    * 在标题+内容的格式中，难以确定 query 的范围
    * 可能影响模型提出问题的能力

## 6. LLM Greedy Decoding 的局限性
* **问题**:  实际应用中，为什么大家不使用 greedy decoding，或者说 greedy decoding 是最优策略吗
* **观点**:  greedy decoding 容易出现重复生成的问题，尤其在代码生成等场景下
* **改进**: 可以使用 preference optimization 的思路来降低 greedy decoding 的重复生成问题
* **相关工作**: 
  * **文章**:  [Overcoming Exposure Bias from Sample-and-Rank Decoding in Neural Machine Translation](https://arxiv.org/pdf/2306.03350)
  * **博客**:  [漫谈RNN之：Exposure Bias与RLHF](https://kexue.fm/archives/8128)


## 7.  Dense 模型和 MoE 模型的比较
* **问题**: 同等参数量下，Dense 模型和 MoE 模型的性能比较
* **观点**: 
    * 在相同总参数量下，MoE 模型通常不如 Dense 模型
    * 在相同激活参数量下，MoE 模型比 Dense 模型强
    * MoE 模型的优势在于可以通过路由机制灵活地激活部分专家模型，从而在计算效率和性能之间取得平衡
    * 可以通过改变路由算法的分布来适应不同的计算需求
    * 可以将高频激活的专家模型蒸馏成一个 Dense 模型
* **相关工作**: 
    * Mixtral-8x7B
    * DeepSeekV2 


## 8.  Exposure Bias 在 LLM 中是否依然存在
*  **问题**:  随着模型规模的扩大，Exposure Bias 问题是否依然存在
* **观点**: 
    *  一种可能是随着表征能力增强和泛化能力提升，Exposure Bias 的影响变小
    *  另一种可能是 Autoregressive 模型本身就是真实分布的一致估计量，随着数据量的增多，模型能够收敛到真实分布，从而消除 Exposure Bias
    *  PPO 等强化学习算法可以缓解 Exposure Bias 问题


## 9. arxiv 提交时间与论文曝光率的关系
* **经验**: 
    * 凌晨 2 点（中国时间）提交论文更容易排在前面
    * 论文排名靠前能够获得更高的曝光率，但影响力最终取决于论文本身的价值
    *  大 V 转发能够显著提升论文的曝光率


## 10. LLM 长文本推理加速
* **问题**: 长文本 LLM 的推理速度慢，尤其是在预填充阶段
* **方法**: MInference 利用动态稀疏注意力机制加速长文本预填充阶段，在保持性能几乎不变的情况下，将 1M tokens 的预填充速度提升了 10 倍
* **相关工作**:
    * LLaMA-3-1M
    * GLM-4-1M
    * Yi-200K
    * Phi-3-128K
    * Qwen2-128K
    * **项目**: MInference ([https://aka.ms/MInference](https://aka.ms/MInference)) 